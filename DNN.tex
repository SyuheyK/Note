\documentclass[a4paper,10pt]{jsarticle}
\usepackage{amsmath,amssymb,mathrsfs,amsthm}
\usepackage{framed,color}
\usepackage{newtxtext,newtxmath}
 \usepackage{ascmac}
% ---Set margin---
\setlength{\textheight}{\paperheight}
\setlength{\topmargin}{4.6truemm}
\addtolength{\topmargin}{-\headheight}
\addtolength{\topmargin}{-\headsep}
\addtolength{\textheight}{-60truemm}

% Theorem Environments ---------------------------------------------------
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}


% ShortCut Environments ---------------------------------------------------
\newcommand{\eq}[1]{\begin{align}#1\end{align}}
\newcommand{\items}[1]{\begin{itemize}#1\end{itemize}}
\newcommand{\enums}[1]{\begin{enumerate}#1\end{enumerate}}
\newcommand{\pmat}[1]{\begin{pmatrix}#1\end{pmatrix}}
\newcommand{\point}[1]{\subsubsection*{#1}\addcontentsline{toc}{subsubsection}{#1}}
\newcommand{\dcup}[1][]{\operatorname*{\cup_\mathnormal{#1}}}
\newcommand{\dcap}[1][]{\operatorname*{\cap_\mathnormal{#1}}}
%
% Commands ------------------------------------------------
\newcommand{\argmin}{\mathop{\rm arg~min}}
%
%
\begin{document}
\setcounter{section}{1}
%%%%%%%%%
%Chapter.2
%%%%%%%%%
\section{順伝播型ネットワーク}
ニューラルネットワークを定式化
%
%section2.1
\subsection{ユニットの出力}
\begin{itembox}[l]{キーワード}
順伝播型(ニューラル)ネットワーク，多層パーセプトロン，重み，バイアス，活性化関数
\end{itembox}
ネットワークを構成する各ユニットは，入力$x_1,...,x_I$を受けとり，総入力:
\eq{u=w_1x_1+\cdots+w_Ix_I+b}
を計算した上で，\text{活性化関数}$f$を用いて
\eq{z=f(u)}
を出力します．
ここで$w_1,...,w_I$は\textbf{重み}，$b$は\textbf{バイアス}です．

$1$ つの層に複数のユニットが存在している場合，ベクトルを用いて次のように書けます．
\eq{\mathbf{u}&=\mathbf{Wx}+\mathbf{b}\\
\mathbf{z}&=\mathbf{f}(\mathbf{u})}
ただし，入力(=第一層)のユニット数は$I$，第二層のユニット数は$J$とし，各記号は次のとおり：
\eq{\mathbf{u}=(u_1,...,u_J)^\top,\mathbf{x}=(x_1,...,x_I)^\top,\mathbf{b}=(b_1,...,b_J)^\top,\mathbf{z}=(z_1,...,z_J)^\top,\\
\mathbf{W}=\begin{pmatrix}(w_{11}&\cdots&w_{1I}\\\vdots&\ddots&\vdots\\w_{J1}&\cdots&w_{JI}\\\end{pmatrix},\mathbf{f}(\mathbf{u})=(f(u_1),...,f(u_J))^\top}

%
%section2.2
\subsection{活性化関数(ユニット間，出力層ではない)}
\begin{itembox}[l]{キーワード}
ロジスティック(シグモイド)関数，シグモイド関数，正規化線形関数，マックスアウト
\end{itembox}
\items{
	\item ロジスティック関数 $\subset$ シグモイド関数　
		\eq{f(u)=\frac{1}{1+e^{-u}}}
	\item 双曲線正接関数(hyperbolic tangent) $\subset$ シグモイド関数　
		\eq{f(u)=\tanh(u)}
	\item 正規化線形関数
		\eq{f(u)=\max(u,0)}
		この関数をもつユニットを\textbf{ReLU} と言います．
	\item 線形写像・恒等写像
	\item ロジスティック関数を区分線形近似したもの
		\eq{f(u)=\begin{cases}-1&u<-1\\u&-1\le u<1\\1&u\ge1\end{cases}}
	\item マックスアウト($K$個のユニットをまとめたようなもの)
		\eq{
			u_{jk}&=\sum_iw_{jik}z_i+b_{jk}\\
			f(u_j)&=\max_{k+1,...,K}u_{jk}}
	}
%
%section2.3
\subsection{多層ネットワーク}
\begin{itembox}[l]{キーワード}
入力層，中間層(=隠れ層)，出力層
\end{itembox}
各層のユニットの入出力の計算を区別するために，層が$l=1,2,...,L$となっているときに各変数の肩に$(l)$と記すことにします．
すると，層$2$のユニットの出力$\mathbf{z}^{(2)}$は，入力層$\mathbf{x}$から
\eq{
	\mathbf{u}^{(2)}&=\mathbf{W}^{(2)}\mathbf{x}+\mathbf{b}^{(2)}\\
	\mathbf{z}^{(2)}&=\mathbf{f}(\mathbf{u}^{(2)})}
と計算され，その後の層$l+1$のユニットの出力$\mathbf{z}^{(l+1)}$は，$1$つ下の層の出力$\mathbf{z}^{(l)}$から
\eq{
	\mathbf{u}^{(l+1)}&=\mathbf{W}^{(l+1)}\mathbf{z}^{(l)}+\mathbf{b}^{(l+1)}\\
	\mathbf{z}^{(l+1)}&=\mathbf{f}(\mathbf{u}^{(l+1)})}
と計算される．最後に層$L$まで計算が終わったら，ネットワークの最終的な出力を
\eq{\mathbf{y}=\mathbf{z}^{(L)}}
と表記する．
各層の重み$\mathbf{W}^{(l)}$とバイアス$\mathbf{b}^{(l)}$をユニットのパラメータと呼び，
出力$\mathbf{y}$は入力とパラメータによって決まることから，
\eq{\mathbf{y}\left(\mathbf{x};\mathbf{W}^{(2)},...,\mathbf{W}^{(L)},\mathbf{b}^{(2)},...,\mathbf{b}^{(L)}\right)}
あるいは簡単に
\eq{\mathbf{y}(\mathbf{x};\mathbf{w})}
と書きます．
%
%section2.4
\subsection{出力層の設計と誤算関数}
\begin{itembox}[l]{キーワード}
訓練サンプル，訓練データ，誤算関数，回帰，二値分類，多クラス分類，最尤推定，ソフトマックス関数，交差エントロピー
\end{itembox}
学習に使用する入出力データ
\eq{\mathcal{D}=\left\{(\mathbf{x}_1,\mathbf{d}_1),(\mathbf{x}_2,\mathbf{d}_2),...,(\mathbf{x}_N,\mathbf{d}_N)\right\}}
を訓練データといいます．$\mathbf{x}_n$が入力，$\mathbf{d}_n$が出力．
%
\subsubsection{回帰}
出力層の活性化関数には恒等写像を用い，
誤算関数には二乗誤算の和
\eq{E(\mathbf{w})&=\frac{1}{2}\sum_{n=1}^N||\mathbf{d}_n-\mathbf{y}(\mathbf{x}_n;\mathbf{w})||^2}
を使用し，$E(\mathbf{w})$がもっとも小さくなる$\mathbf{w}$を求めます．
%
\subsubsection{二値分類}
ここでは$\mathbf{x}$を指定したときに$d=1$となる事後確率$p(d=1|\mathbf{x})$を，
ネットワーク全体の出力$y(\mathbf{x};\mathbf{w})$でモデル化し，
出力層の活性化関数にロジスティック関数を使うことで，
\eq{p(d=1|\mathbf{x})\approx y(\mathbf{x};\mathbf{w})}
とします．
パラメータ$\mathbf{w}$は最尤推定で求めることにします．
$p(d=1|\mathbf{x})=y(\mathbf{x};\mathbf{w})$より$p(d=0|\mathbf{x})=1-y(\mathbf{x};\mathbf{w})$であることと，
\eq{p(d|\mathbf{x})=p(d=1|\mathbf{x})^dp(d=0|\mathbf{x})^{1-d}}
より，$N$個の入力に対する尤度は
\eq{L(\mathbf{w})=\prod_{n=1}^Np(d_n|\mathbf{x}_n;\mathbf{w})=\prod_{n=1}^Ny(\mathbf{x}_n;\mathbf{w})^{d_n}\left\{1-y(\mathbf{x}_n;\mathbf{w})\right\}^{1-d_n}}
となります．誤差関数としてはこれに対数をとって符号を反転させた
\eq{E(\mathbf{w})=-\sum_{n=1}^N\left[d_n\log y(\mathbf{x}_n;\mathbf{w})+(1-d_n)\log\left\{1-y(\mathbf{x}_n;\mathbf{w})\right\}\right]}
を用て，パラメータ$\mathbf{w}$を求めます．
%
\subsubsection{多クラス分類}
\subsubsection*{出力層の活性化関数}
ネトワークの出力層にクラス数$K$と同じ$K$個のユニットを並べ，出力層の各ユニット$k(=1,...,K)$の出力を
\eq{y_k=z_k^{(L)}=\frac{\exp(u_k^{(L)})}{\sum_{j=1}^K\exp(u_j^{(L)})}}
とします．この関数はソフトマックス関数と呼ばれます．
この式のスコア$u_k^{(L)}$に対して
\eq{u_k^{(L)}&=\mathbf{W}^{(L)}\mathbf{z}^{(L)}+\mathbf{b}^{(L)}}
と出力層のユニットを当てはめ，$\mathbf{W}$を求めます．

\subsubsection*{誤差関数の導出}
分類する$K$個のクラスを$\mathcal{C}_1,...,\mathcal{C}_K$を表すとき，ユニット$k$の出力値$y_k$を用いて，クラス$\mathcal{C}_k$に属する確率を
\eq{p(\mathcal{C}_k|\mathbf{x})=y_k=z_k^{(L)}}
でモデル化します．
$n$個めのサンプルがクラス$\mathcal{C}_k$に入っているとき，
出力値であるラベル$\mathbf{d}_n$が$k$番目の要素が$1$でそれ以外は$0$であるベクトルで表されているとします．
たとえば，合計$10$クラスで$3$番目のクラスに属しているとき，
\eq{\mathbf{d}_n=[0~0~1~0~0~0~0~0~0~0]^\top}
と表されます．このとき$\mathbf{d}_n$の事後分布は
\eq{p(\mathbf{d}_n|\mathbf{x})=\prod_{k=1}^Kp(\mathcal{C}_k|\mathbf{x})^{d_k}}
となります．
これをもとにした尤度関数$L(\mathbf{w})$は
\eq{L(\mathbf{w})
	&=\prod_{n=1}^Np(\mathbf{d}_n|\mathbf{x}_n;\mathbf{w})
	=\prod_{n=1}^N\prod_{k+1}^Kp(\mathcal{C}_n|\mathbf{x}_n)^{d_{n,k}}
	=\prod_{n=1}^N\prod_{k+1}^K(y_k(\mathbf{x};\mathbf{w}))^{d_{n,k}}}
となるので，対数と負をとったものを誤差関数とします：
\eq{E(\mathbf{w})=-\sum_{n=1}^N\sum_{k+1}^Kd_{n,k}\log y_k(\mathbf{x};\mathbf{w})}
この誤差関数は交差エントロピーと呼ばれます．

%
%chapter 2の補足
\subsubsection{出力層の活性化関数}
二値分類でロジスティック関数，多クラス分類でソフトマックス関数を使うことの妥当性を確認する計算
\subsubsection*{二値分類}
事後確率$p(d=1|\mathbf{x})$は条件付確率の定義から
\eq{p(d=1|\mathbf{x})
	&=\frac{p(\mathbf{x},d=1)}{p(\mathbf{x})}
	=\frac{p(\mathbf{x},d=1)}{p(\mathbf{x},d=0)+p(\mathbf{x},d=1)}}
と計算できます．ここでスコア$u$を
\eq{u:=\log\frac{p(\mathbf{x},d=1)}{p(\mathbf{x},d=0)}}
で表せば，先ほどの事後確率は
\eq{p(d=1|\mathbf{x})
	&=\frac{p(\mathbf{x},d=1)}{p(\mathbf{x},d=0)+p(\mathbf{x},d=1)}\\
	&=\frac{\mathrm{e}^{\log p(\mathbf{x},d=1)}}{\mathrm{e}^{\log p(\mathbf{x},d=0)}+\mathrm{e}^{\log p(\mathbf{x},d=1)}}\\
	&=\frac{1}{1+\mathrm{e}^{\log p(\mathbf{x},d=0) - \log p(\mathbf{x},d=1)}}\\
	&=\frac{1}{1+\mathrm{e}^{-u}}}
と計算でき，ロジスティック関数に一致することを確認できます．
\subsubsection*{多クラス分類}
ロジスティック関数と同様クラス$\mathcal{C}_k$の事後確率は条件付確率の定義から
\eq{p(\mathcal{C}_k|\mathbf{x})
	&=\frac{p(\mathbf{x},\mathcal{C}_k)}{p(\mathbf{x})}=\frac{p(\mathbf{x},\mathcal{C}_k)}{\sum_{j=1}^Kp(\mathbf{x},\mathcal{C}_j)}}
となります．ここではクラス$k$のスコアを$u_k:=\log p(\mathbf{x},\mathcal{C}_k)$と置くことで，この事後確率を
\eq{p(\mathcal{C}_k|\mathbf{x})
	&=\frac{p(\mathbf{x},\mathcal{C}_k)}{\sum_{j=1}^Kp(\mathbf{x},\mathcal{C}_j)}\\
	&=\frac{\mathrm{e}^{\log p(\mathbf{x},\mathcal{C}_k)}}{\sum_{j=1}^K\mathrm{e}^{\log p(\mathbf{x},\mathcal{C}_j)}}\\
	&=\frac{\mathrm{e}^{u_k}}{\sum_{j=1}^K\mathrm{e}^{u_j}}}
と計算でき，ソフトマックス関数に一致することを確認できます．

%%%%%%%%%
%Chapter.3前半&4
%%%%%%%%%
\section{確率的勾配降下法と誤差逆伝播法}
%
%section3.1
\subsection{勾配降下法}
\begin{itembox}[l]{キーワード}
大域解，局所解，勾配効果法，勾配，学習係数
\end{itembox}
\subsubsection*{やることまとめ}
順伝播型ネットワークの学習のまとめ：与えられた訓練データ
\eq{\mathcal{D}=\left\{(\mathbf{x}_1,\mathbf{d}_1),(\mathbf{x}_2,\mathbf{d}_2),...,(\mathbf{x}_N,\mathbf{d}_N)\right\}}
を元に計算される誤差関数$E(\mathbf{w})$をネットワークのパラメータ$\mathbf{w}$について最小化すること．
学習のゴールは$\argmin_\mathbf{w}E(\mathbf{w})$(=選んだ$E(\mathbf{w})$に対し最小値を与える$\mathbf{w}$)を求めることです．
ただし，$E(\mathbf{w})$は一般に凸関数ではないため，大域的最小解を直接求めるのは不可能です．
代わりに，$E(\mathbf{w})$の局所的な最小解$\mathbf{w}$を求めます．
この$E(\mathbf{w})$の局所的最小解$\mathbf{w}$を求める方法の代表例が勾配降下法です．
\subsubsection*{勾配降下法とは}
勾配降下法とは，現在の$\mathbf{w}$を，負の勾配方向$-\nabla E$に少し動かすことを繰り返す方法です
\footnote{$\nabla E$は，変数である$\mathbf{w}$を動かしたとき$E$は$\nabla E$だけ増加するということを表す量のことです．これを負の方向に動かせば$E$は$\nabla E$だけ減少します．}．
ここで勾配とは微分を並べたベクトル：
\eq{\nabla E=\frac{\partial E}{\partial\mathbf{w}}=\left[\frac{\partial E}{\partial w_1}\cdot\frac{\partial E}{\partial w_M}\right]^\top}
です．現在の重みを$\mathbf{w}^{(t)}$，動かした後の重みを$\mathbf{w}^{(t+1)}$と表せば，
\eq{\mathbf{w}^{(t+1)}=\mathbf{w}^{(t+1)}-\epsilon\nabla E\label{renewal1}}
で重みを更新することが勾配降下法であると書き直すことができます．
$\epsilon$は$\mathbf{w}$の更新量の大きさを決めるパラメータ({\bf 学習係数})です\footnote{これは人間が決めます．}．
実際の更新手順は，初期値$\mathbf{w}^{(1)}$を適当に決め，式(\ref{renewal1})で$\mathbf{w}^{(2)},\mathbf{w}^{(3)},...$と順次計算していきます．
%
%section3.2
\subsection{確率的勾配降下法}
\begin{itembox}[l]{キーワード}
バッチ学習，確率的勾配降下法
\end{itembox}
\subsubsection*{確率的勾配降下法とは}
前節の誤差関数は全サンプルを用いたものでした\footnote{全サンプルを使用する学習を\textbf{バッチ学習}と言ったりします}．
$n$番目のサンプル1個だけの誤差を$E_n(\mathbf{w})$と表せば
\eq{E(\mathbf{w})=\sum_{n=1}^NE_n(\mathbf{w})}
となります．確率的勾配効果法は更新式(\ref{renewal1})の勾配$\nabla E$を$\nabla E_n$に置き換えた更新式：
\eq{\mathbf{w}^{(t+1)}=\mathbf{w}^{(t+1)}-\epsilon\nabla E_\mathbf{n}\label{renewal2}}
を用いる方法です．
\subsubsection*{確率的勾配降下法のメリット}
\items{
	\item 訓練データに冗長性がある場合，計算効率が上昇する
	\item 一つの局所的最小解にハマるリスクが下がる
	\item 学習の経過を細かく監視できる
	\item オンライン学習が可能
}
%
%section4.1
\subsection{勾配計算の難しさ}
前節の勾配を試しに，
二乗誤差：$E_n=1/2||\mathbf{y}(\mathbf{x}_n)-\mathbf{d}_n||^2$に対して，
第$l$層の重み$w_{ij}^{(l)}$に関する微分で計算してみましょう．
合成関数の微分から
\eq{\frac{\partial E_n}{\partial w_{ij}^{(l)}}=(\mathbf{y}(\mathbf{x}_n)-\mathbf{d}_n)^\top\frac{\partial\mathbf{y}}{\partial w_{ij}^{(l)}}}
と計算できます．
さらに中身の微分$\partial\mathbf{y} / \partial w_{ij}^{(l)}$も計算できれば，この微分を数値的にも計算できます．
だがしかし，第$L$層である出力層から遡って計算するため，
微分をする対象の$\mathbf{y}$は
\eq{\mathbf{y}
	&=\mathbf{f}\left(\mathbf{u}^{(L)}\right)\\
	&=\mathbf{f}\left(\mathbf{W}^{(L)}\mathbf{z}^{(L-1)}+\mathbf{b}^{(L)}\right)\\
	&=\mathbf{f}\left(\mathbf{W}^{(L)}\mathbf{f}(\mathbf{W}^{(L-1)}\mathbf{z}^{(L-2)}+\mathbf{b}^{(L-1)})+\mathbf{b}^{(L)}\right)\\
	&=\mathbf{f}\left(\mathbf{W}^{(L)}\mathbf{f}(\mathbf{W}^{(L-1)}\mathbf{f}(\cdots\mathbf{f}(\mathbf{W}^{(l)}\mathbf{z}^{(l-1)}+\mathbf{b}^{(l)})\cdots))+\mathbf{b}^{(L)}\right)\\
	}
となり，上式の$\mathbf{W}^{(l)}$の一要素である$w_{ij}^{(l)}$で微分しなければならないですが，
ご覧のとおり多くの活性化関数の入れ子の中に$w_{ij}^{(l)}$があるので連鎖規則を何度も適用する必要があり，この微分を計算するのは容易ではありません．
これを解決するのが次節から説明する\textbf{誤差逆伝播法}です．
%
%section4.2
\subsection{回帰の2層ネットワークでの誤差逆伝播法の計算例}

%
%section4.3
\subsection{多層ネットワークでの誤差逆伝播法(一般化)}

%
%section4.4
\subsection{勾配降下法のアルゴリズム完全版}

%%%%%%%%%
%Chapter.3後半&4.5
%%%%%%%%%
\section{パラメータ推計に関するトピック}
%
%section3.3
\subsection{ミニバッチの利用}
%
%section3.4
\subsection{汎化性能と過剰適合}
%
%section3.5
\subsection{過剰適合の緩和}
%
%section3.6
\subsection{学習のトリック}
%
%section4.5
\subsection{勾配消失問題}

\end{document}

















